{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e64de5b",
   "metadata": {},
   "source": [
    "# What we want:\n",
    "\n",
    "### A single number 1 -- 12 that describes the quality of a given observation.\n",
    "\n",
    "This number should take into account:\n",
    "1. Goodness of seeing, both from scintillation and from HSM/MFGS parameters\n",
    "2. Duration of observations. Longer is better.\n",
    "3. Interesting events \"nearby\"\n",
    "4. Optionally: IRIS pointing in the same time span.\n",
    "\n",
    "## The Quality is for the full observing *day*, not per pointing.\n",
    "\n",
    "#### Breaking it down:\n",
    "1. Goodness of seeing:\n",
    "    a. Scrape the DCSS log or FIRS files for scintillation monitor values\n",
    "    b. Read in the ROSA/Zyla seeing params, and determine the spread in these values\n",
    "    c. Place these values in a csv file, with observation type.\n",
    "2. Duration of observations.\n",
    "    a. Read by start and end time in FITS files. The flow chart to check:\n",
    "        i. ROSA\n",
    "        ii. FIRS\n",
    "        iii. SPINOR/HSG\n",
    "        iv. NEVER Zyla\n",
    "    b. Total obs time in the same csv file\n",
    "3. Interesting events \"nearby\"\n",
    "    a. Read in the pointing_info.txt file.\n",
    "        i. If the number of pointings and starttimes is the same, use the obssum start/end times\n",
    "    b. Query HEK for coronal rain, filament eruptions, and flares.\n",
    "    c. Check for anything from the pointing center to a max radius of 200\" (FOV is 180\")\n",
    "    d. If it's within 100\", with a starttime that's between our start and end, add 2 to metric\n",
    "    e. If it satisfies one condition but not the other, add 1\n",
    "    f: If it is outside 100\" and the starttime is within 30 minutes of our window, add 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7387a0",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "##### Is it worth going back to my obssum code, and modifying it to write the endtime as well? It would save some time....\n",
    "\n",
    "###### I did it...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85d90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7dc52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from sunpy.coordinates import frames\n",
    "from sunpy.net import Fido, attrs as a\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc32f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'savefig.dpi':300,\n",
    "    'axes.labelsize':12,\n",
    "    'axes.labelweight':'bold',\n",
    "    'axes.titleweight':'bold',\n",
    "    'figure.titleweight':'bold',\n",
    "    'axes.titlesize':14,\n",
    "    'font.size':12,\n",
    "    'legend.fontsize':12,\n",
    "    'font.family':'serif'\n",
    "}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c7c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_metaparams(indir, date):\n",
    "    \"\"\"Sets up information needed for our quality metrics.\n",
    "    Outputs a structure with:\n",
    "    OBSTYPE\n",
    "    DURATION\n",
    "    HSM Coefficient of Variation (GBAND)\n",
    "    HSM Coefficient of Variation (4170)\n",
    "    HSM Coefficient of Variation (CAK)\n",
    "    MFGS Coefficient of Variation (ZYLA)\n",
    "    (CV is stdev/mean)\n",
    "    EVENT FACTOR\n",
    "    EVENT FACTOR TYPE\n",
    "    \"\"\"\n",
    "    #First, check if there's pointing info\n",
    "    pinfo = os.path.join(os.path.join(indir, \"context_ims\"),\"pointing_info.txt\")\n",
    "    if not os.path.isfile(pinfo):\n",
    "        return None\n",
    "    pointings = np.recfromtxt(pinfo, delimiter=',',names=True,encoding=None)\n",
    "    pointings = np.atleast_1d(pointings)\n",
    "    if len(pointings.TIME) == 0:\n",
    "        return None\n",
    "    # Cool. There's pointing info.\n",
    "    # Now, check if there's seeing info.\n",
    "    all_seeing = sorted(glob.glob(os.path.join(indir, \"*seeing_quality.npy\")))\n",
    "    if len(all_seeing) == 0:\n",
    "        return None\n",
    "    # Cool. There's seeing info.\n",
    "    \n",
    "    # OBSTYPE\n",
    "    obsreg = open(\"/var/www/html/observations_registry.csv\",\"r\")\n",
    "    oreg_lines = obsreg.readlines()\n",
    "    obsreg.close()\n",
    "    all_datestrs = [l.split(\",\")[1] for l in oreg_lines]\n",
    "    all_otypes = [l.split(\",\")[2] for l in oreg_lines]\n",
    "    obstype = all_otypes[all_datestrs.index(date)]\n",
    "    \n",
    "    duration = np.timedelta64(0,\"m\")\n",
    "    for i in range(len(pointings.TIME)):\n",
    "        duration += np.datetime64(pointings.END[i]) - np.datetime64(pointings.TIME[i])\n",
    "    \n",
    "    zyla_cv = []\n",
    "    gband_cv = []\n",
    "    cak_cv = []\n",
    "    cont_cv = []\n",
    "    ibis_cv = []\n",
    "    \n",
    "    for i in range(len(all_seeing)):\n",
    "        if \"zyla\" in all_seeing[i]:\n",
    "            see = np.load(all_seeing[i])\n",
    "            zyla_cv.append(np.nanstd(see['MFGS'])/np.nanmean(see['MFGS']))\n",
    "        else:\n",
    "            try:\n",
    "                see = np.load(all_seeing[i])\n",
    "            except:\n",
    "                see = np.load(all_seeing[i],allow_pickle=True).flat[0]\n",
    "            if 'gband' in all_seeing[i]:\n",
    "                gband_cv.append(np.nanstd(see['HSM'])/np.nanmean(see['HSM']))\n",
    "            elif '4170_' in all_seeing[i]:\n",
    "                cont_cv.append(np.nanstd(see['HSM'])/np.nanmean(see['HSM']))\n",
    "            elif 'cak_' in all_seeing[i]:\n",
    "                cak_cv.append(np.nanstd(see['HSM'])/np.nanmean(see['HSM']))\n",
    "            elif 'ibis' in all_seeing[i]:\n",
    "                ibis_cv.append(np.nanstd(see['HSM'])/np.nanmean(see['HSM']))\n",
    "                \n",
    "    if len(zyla_cv) == 0:\n",
    "        zyla_cv = np.nan\n",
    "    else:\n",
    "        zyla_cv = np.nanmean(np.array(zyla_cv))\n",
    "    if len(gband_cv) == 0:\n",
    "        gband_cv = np.nan\n",
    "    else:\n",
    "        gband_cv = np.nanmean(np.array(gband_cv))\n",
    "    if len(cak_cv) == 0:\n",
    "        cak_cv = np.nan\n",
    "    else:\n",
    "        cak_cv = np.nanmean(np.array(cak_cv))\n",
    "    if len(cont_cv) == 0:\n",
    "        cont_cv = np.nan\n",
    "    else:\n",
    "        cont_cv = np.nanmean(np.array(cont_cv))\n",
    "    if len(ibis_cv) == 0:\n",
    "        ibis_cv = np.nan\n",
    "    else:\n",
    "        ibis_cv = np.nanmean(np.array(cont_cv))\n",
    "    # Cool. We've got duration and seeing. \n",
    "    # Now for the hard part.\n",
    "    # Our fudge factor for events\n",
    "    \n",
    "    # Conceivably, there might be multiple flares/other events in frame.\n",
    "    # In this case, we want the max fudge factor, so append to a list\n",
    "    # NOTE: Coronal rain doesn't necessarily have an associated time.\n",
    "    # For CR, if there's an event, it's close enough temporally by default.\n",
    "    event_factors = []\n",
    "    events = []\n",
    "    see_cv = []\n",
    "    for i in range(len(pointings.TIME)):\n",
    "        tele_pointing = SkyCoord(\n",
    "            pointings['SLON'][i]*u.deg,\n",
    "            pointings['SLAT'][i]*u.deg,\n",
    "            frame=frames.HeliographicStonyhurst,\n",
    "            observer='earth',\n",
    "            obstime=pointings['TIME'][i]\n",
    "        )\n",
    "        tele_pointing = tele_pointing.transform_to(frames.Helioprojective)\n",
    "        tele_start = np.datetime64(pointings.TIME[i])\n",
    "        tele_end = np.datetime64(pointings.END[i])\n",
    "\n",
    "        #### Real quick, add in scmon values...\n",
    "        see_cv.append(float(pointings.SSCIN[i])/float(pointings.MSCIN[i]))\n",
    "        \n",
    "        #### Flares first.\n",
    "        #### ...Cause I care about them the most.\n",
    "        flare_fudge = []\n",
    "        flare_events = []\n",
    "        flare_query = Fido.search(\n",
    "            a.Time(\n",
    "                (tele_start - np.timedelta64(30,'m')).astype(str),\n",
    "                (tele_end + np.timedelta64(30,'m')).astype(str)\n",
    "            ),\n",
    "            a.hek.EventType('FL'),\n",
    "            a.hek.FL.GOESCls>'B1.0'\n",
    "        )\n",
    "        if len(flare_query['hek']) > 0:\n",
    "            flare_xcd = flare_query['hek']['event_coord1']\n",
    "            flare_ycd = flare_query['hek']['event_coord2']\n",
    "            flare_peak = flare_query['hek']['event_peaktime']\n",
    "            flare_class = flare_query['hek']['fl_goescls']\n",
    "            flare_coordsys = flare_query['hek']['event_coordunit']\n",
    "        else:\n",
    "            flare_xcd = []\n",
    "            flare_ycd = []\n",
    "            flare_peak = []\n",
    "            flare_class = []\n",
    "            flare_coordsys = []\n",
    "        for j in range(len(flare_xcd)):\n",
    "            if flare_coordsys[j] == 'degrees':\n",
    "                unit=u.deg\n",
    "                frm = frames.HeliographicStonyhurst\n",
    "            elif flare_coordsys[j] == 'arcsec':\n",
    "                unit=u.arcsec\n",
    "                frm=frames.Helioprojective\n",
    "            flare_coord = SkyCoord(\n",
    "                flare_xcd[j]*unit,\n",
    "                flare_ycd[j]*unit,\n",
    "                obstime=flare_peak[j],\n",
    "                observer='earth',\n",
    "                frame=frm\n",
    "            )\n",
    "            flare_coord = flare_coord.transform_to(frames.Helioprojective)\n",
    "            del_x = float((tele_pointing.Tx - flare_coord.Tx).value)\n",
    "            del_y = float((tele_pointing.Ty - flare_coord.Ty).value)\n",
    "            distance = np.sqrt(del_x**2 + del_y**2)\n",
    "            # Check if it's nearby:\n",
    "            if distance <= 200:\n",
    "                event_time = np.datetime64(flare_peak[j].value[0])\n",
    "                # Best Case. Perfectly in field and on time\n",
    "                if (event_time < tele_end) & (event_time > tele_start) & (distance <= 100):\n",
    "                    flare_fudge.append(2)\n",
    "                    flare_events.append(flare_class[j])\n",
    "                # Second best case. Slightly out of time, but perfectly in field.\n",
    "                elif (distance <= 100):\n",
    "                    flare_fudge.append(1)\n",
    "                    flare_events.append(flare_class[j])\n",
    "                # Second best case. Perfectly in time, slightly out of field.\n",
    "                elif (event_time < tele_end) & (event_time > tele_start):\n",
    "                    flare_fudge.append(1)\n",
    "                    flare_events.append(flare_class[j])\n",
    "                # We've already passively selected this. Just out of FOV, and not in time.\n",
    "                else:\n",
    "                    flare_fudge.append(0.5)\n",
    "                    flare_events.append(flare_class[j])\n",
    "        # Check our flares. If there are any, append them to the event list.\n",
    "        if len(flare_fudge) > 0:\n",
    "            event_factors.append(np.nanmax(flare_fudge))\n",
    "            events.append(flare_events[flare_fudge.index(np.nanmax(flare_fudge))])\n",
    "        \n",
    "        #### Filament eruptions Second.\n",
    "        #### ...Cause they're like flares.\n",
    "        filament_fudge = []\n",
    "        filament_query = Fido.search(\n",
    "            a.Time(\n",
    "                (tele_start - np.timedelta64(30,'m')).astype(str),\n",
    "                (tele_end + np.timedelta64(30,'m')).astype(str)\n",
    "            ),\n",
    "            a.hek.EventType('FE')\n",
    "        )\n",
    "        if len(filament_query['hek']) > 0:\n",
    "            filament_xcd = filament_query['hek']['event_coord1']\n",
    "            filament_ycd = filament_query['hek']['event_coord2']\n",
    "            filament_peak = filament_query['hek']['event_peaktime']\n",
    "            filament_coordsys = filament_query['hek']['event_coordunit']\n",
    "        else:\n",
    "            filament_xcd = []\n",
    "            filament_ycd = []\n",
    "            filament_peak = []\n",
    "            filament_coordsys = []\n",
    "        for j in range(len(filament_xcd)):\n",
    "            if filament_coordsys[j] == 'degrees':\n",
    "                unit=u.deg\n",
    "                frm = frames.HeliographicStonyhurst\n",
    "            elif filament_coordsys[j] == 'arcsec':\n",
    "                unit=u.arcsec\n",
    "                frm=frames.Helioprojective\n",
    "            filament_coord = SkyCoord(\n",
    "                filament_xcd[j]*unit,\n",
    "                filament_ycd[j]*unit,\n",
    "                obstime=filament_peak[j],\n",
    "                observer='earth',\n",
    "                frame=frm\n",
    "            )\n",
    "            filament_coord = filament_coord.transform_to(frames.Helioprojective)\n",
    "            del_x = float((tele_pointing.Tx - filament_coord.Tx).value)\n",
    "            del_y = float((tele_pointing.Ty - filament_coord.Ty).value)\n",
    "            distance = np.sqrt(del_x**2 + del_y**2)\n",
    "            # Check if it's nearby:\n",
    "            if distance <= 200:\n",
    "                event_time = np.datetime64(filament_peak[j].value[0])\n",
    "                # Best Case. Perfectly in field and on time\n",
    "                if (event_time < tele_end) & (event_time > tele_start) & (distance <= 100):\n",
    "                    filament_fudge.append(2)\n",
    "                # Second best case. Slightly out of time, but perfectly in field.\n",
    "                elif (distance <= 100):\n",
    "                    filament_fudge.append(1)\n",
    "                # Second best case. Perfectly in time, slightly out of field.\n",
    "                elif (event_time < tele_end) & (event_time > tele_start):\n",
    "                    filament_fudge.append(1)\n",
    "                # We've already passively selected this. Just out of FOV, and not in time.\n",
    "                else:\n",
    "                    filament_fudge.append(0.5)\n",
    "        # Check our filaments. If there are any, append them to the event list.\n",
    "        if len(filament_fudge) > 0:\n",
    "            event_factors.append(np.nanmax(filament_fudge))\n",
    "            events.append('Filament Eruption')\n",
    "            \n",
    "    \n",
    "        #### Coronal Rain last\n",
    "        #### Cause it's calm.\n",
    "        rain_fudge = []\n",
    "        rain_query = Fido.search(\n",
    "            a.Time(\n",
    "                (tele_start - np.timedelta64(30,'m')).astype(str),\n",
    "                (tele_end + np.timedelta64(30,'m')).astype(str)\n",
    "            ),\n",
    "            a.hek.EventType('CR')\n",
    "        )\n",
    "        if len(rain_query['hek']) > 0:\n",
    "            rain_xcd = rain_query['hek']['event_coord1']\n",
    "            rain_ycd = rain_query['hek']['event_coord2']\n",
    "            rain_coordsys = rain_query['hek']['event_coordunit']\n",
    "        else:\n",
    "            rain_xcd = []\n",
    "            rain_ycd = []\n",
    "            rain_coordsys = []\n",
    "        for j in range(len(rain_xcd)):\n",
    "            if rain_coordsys[j] == 'degrees':\n",
    "                unit=u.deg\n",
    "                frm = frames.HeliographicStonyhurst\n",
    "            elif rain_coordsys[j] == 'arcsec':\n",
    "                unit=u.arcsec\n",
    "                frm=frames.Helioprojective\n",
    "            rain_coord = SkyCoord(\n",
    "                rain_xcd[j]*unit,\n",
    "                rain_ycd[j]*unit,\n",
    "                obstime=tele_start.astype(str),\n",
    "                observer='earth',\n",
    "                frame=frm\n",
    "            )\n",
    "            rain_coord = rain_coord.transform_to(frames.Helioprojective)\n",
    "            del_x = float((tele_pointing.Tx - rain_coord.Tx).value)\n",
    "            del_y = float((tele_pointing.Ty - rain_coord.Ty).value)\n",
    "            distance = np.sqrt(del_x**2 + del_y**2)\n",
    "            # Check if it's nearby:\n",
    "            if distance <= 200:\n",
    "                event_time = np.datetime64(rain_peak[j].value[0])\n",
    "                # Best Case. Perfectly in field and on time\n",
    "                if (distance <= 100):\n",
    "                    rain_fudge.append(2)\n",
    "                # Second best case. Slightly out of time, but perfectly in field.\n",
    "                else:\n",
    "                    rain_fudge.append(1)\n",
    "        # Check our rains. If there are any, append them to the event list.\n",
    "        if len(rain_fudge) > 0:\n",
    "            event_factors.append(np.nanmax(rain_fudge))\n",
    "            events.append('Coronal Rain')\n",
    "    \n",
    "    if len(event_factors) > 0:\n",
    "        event_correction = np.nanmax(event_factors)\n",
    "        event_correction_type = events[event_factors.index(np.nanmax(event_factors))]\n",
    "    else:\n",
    "        event_correction = 0\n",
    "        event_correction_type = 'None'\n",
    "        \n",
    "    #### Putting it all together...\n",
    "    if len(see_cv) == 0:\n",
    "        see_cv = np.nan\n",
    "    else:\n",
    "        see_cv = np.nanmean(np.array(see_cv))\n",
    "    \n",
    "    data_quality = {\n",
    "        'OBSTYPE':obstype,\n",
    "        'DURATION':duration.astype('timedelta64[m]').astype(int),\n",
    "        'ZYLA_CV':zyla_cv,\n",
    "        'CONT_CV':cont_cv,\n",
    "        'GBAND_CV':gband_cv,\n",
    "        'CAK_CV':cak_cv,\n",
    "        'IBIS_CV':ibis_cv,\n",
    "        'SEE_CV':see_cv,\n",
    "        'EVENT_FACTOR':event_correction,\n",
    "        'EVENT_TYPE':event_correction_type\n",
    "    }\n",
    "    return data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96aa48db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▍                                        | 14/417 [00:25<17:08,  2.55s/it]/home/sellers/.conda/envs/ssoc/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_11584/4105711041.py:50: RuntimeWarning: Mean of empty slice\n",
      "  zyla_cv.append(np.nanstd(see['MFGS'])/np.nanmean(see['MFGS']))\n",
      "/tmp/ipykernel_11584/4105711041.py:68: RuntimeWarning: Mean of empty slice\n",
      "  zyla_cv = np.nanmean(np.array(zyla_cv))\n",
      "  5%|██                                        | 20/417 [00:57<33:50,  5.11s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      "  5%|██▏                                       | 22/417 [01:07<33:09,  5.04s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 12%|████▉                                     | 49/417 [02:57<29:54,  4.88s/it]/home/sellers/.conda/envs/ssoc/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_11584/4105711041.py:50: RuntimeWarning: Mean of empty slice\n",
      "  zyla_cv.append(np.nanstd(see['MFGS'])/np.nanmean(see['MFGS']))\n",
      "/tmp/ipykernel_11584/4105711041.py:68: RuntimeWarning: Mean of empty slice\n",
      "  zyla_cv = np.nanmean(np.array(zyla_cv))\n",
      " 15%|██████▏                                   | 62/417 [03:50<21:21,  3.61s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 15%|██████▎                                   | 63/417 [03:52<19:01,  3.22s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 18%|███████▎                                  | 73/417 [04:34<31:01,  5.41s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 18%|███████▍                                  | 74/417 [04:38<27:33,  4.82s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 18%|███████▌                                  | 75/417 [04:44<29:57,  5.26s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 20%|████████▎                                 | 82/417 [05:31<45:04,  8.07s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 20%|████████▎                                 | 83/417 [05:42<50:01,  8.99s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 22%|█████████▏                                | 91/417 [06:49<33:14,  6.12s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 22%|█████████▎                                | 92/417 [06:52<27:22,  5.05s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 24%|█████████▉                                | 99/417 [07:39<43:56,  8.29s/it]/tmp/ipykernel_11584/4105711041.py:84: RuntimeWarning: Mean of empty slice\n",
      "  ibis_cv = np.nanmean(np.array(cont_cv))\n",
      " 51%|████████████████████▉                    | 213/417 [15:20<18:03,  5.31s/it]/home/sellers/.conda/envs/ssoc/lib/python3.9/site-packages/astropy/time/formats.py:1395: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  val1_uint32 = val1.view((np.uint32, val1.dtype.itemsize // 4))\n",
      " 67%|███████████████████████████▎             | 278/417 [20:25<07:31,  3.25s/it]/home/sellers/.conda/envs/ssoc/lib/python3.9/site-packages/astropy/time/formats.py:1395: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  val1_uint32 = val1.view((np.uint32, val1.dtype.itemsize // 4))\n",
      " 78%|████████████████████████████████▏        | 327/417 [23:47<04:52,  3.25s/it]/home/sellers/.conda/envs/ssoc/lib/python3.9/site-packages/astropy/time/formats.py:1395: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  val1_uint32 = val1.view((np.uint32, val1.dtype.itemsize // 4))\n",
      " 88%|████████████████████████████████████     | 367/417 [26:34<03:17,  3.96s/it]/home/sellers/.conda/envs/ssoc/lib/python3.9/site-packages/astropy/time/formats.py:1395: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  val1_uint32 = val1.view((np.uint32, val1.dtype.itemsize // 4))\n",
      "100%|█████████████████████████████████████████| 417/417 [30:45<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "all_dirs = sorted(glob.glob(\"/sunspot/solardata/20*/*/*/\"))[1:]\n",
    "all_dates = [d.split(\"/\")[-4] + '-' + d.split(\"/\")[-3] + '-' + d.split(\"/\")[-2] for d in all_dirs]\n",
    "\n",
    "master_file = open(\"quality_params.txt\",\"w\")\n",
    "master_file.write(\"DATE,OBSTYPE,DURATION,ZYLA_CV,CONT_CV,GBAND_CV,CAK_CV,IBIS_CV,SEE_CV,EVENT_FACTOR,EVENT_TYPE\\n\")\n",
    "for i in tqdm.tqdm(range(len(all_dirs))):\n",
    "    try:\n",
    "        dq_dict = quality_metaparams(all_dirs[i], all_dates[i])\n",
    "    except:\n",
    "        dq_dict = None\n",
    "    if dq_dict:\n",
    "        write_list = [all_dates[i]]\n",
    "        for key in list(dq_dict.keys()):\n",
    "            write_list.append(str(dq_dict[key]))\n",
    "        write_str = ','.join(write_list)\n",
    "        master_file.write(\n",
    "            write_str + \"\\n\"\n",
    "        )\n",
    "    else:\n",
    "        write_str = all_dates[i]+',nan,nan,nan,nan,nan,nan,nan,nan,nan,nan\\n'\n",
    "        master_file.write(write_str)\n",
    "master_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f3c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = np.genfromtxt(\"quality_control/quality_params.txt\",\n",
    "                  delimiter=',',\n",
    "                  names=True,\n",
    "                  encoding=None,\n",
    "                  dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253c44f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DATE',\n",
       " 'OBSTYPE',\n",
       " 'DURATION',\n",
       " 'ZYLA_CV',\n",
       " 'CONT_CV',\n",
       " 'GBAND_CV',\n",
       " 'CAK_CV',\n",
       " 'IBIS_CV',\n",
       " 'SEE_CV',\n",
       " 'EVENT_FACTOR',\n",
       " 'EVENT_TYPE')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14979bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pctile_range = np.arange(0,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9b4f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "zyla_cv = qp['ZYLA_CV'][qp['OBSTYPE'] == 'flare']\n",
    "zyla_cv = zyla_cv[~np.isnan(zyla_cv)]\n",
    "\n",
    "dur = qp['DURATION'][qp['OBSTYPE']=='flare']\n",
    "dur = dur[~np.isnan(dur)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf7d50bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.  ,  18.9 ,  23.9 ,  27.21,  29.28,  30.9 ,  32.28,  33.66,\n",
       "        37.64,  41.  ,  41.  ,  43.95,  46.84,  48.91,  50.32,  51.  ,\n",
       "        51.  ,  51.  ,  51.  ,  51.  ,  51.  ,  51.  ,  51.  ,  51.  ,\n",
       "        51.  ,  51.  ,  51.  ,  51.  ,  51.  ,  51.01,  51.7 ,  52.39,\n",
       "        53.08,  53.77,  54.46,  55.6 ,  58.36,  60.06,  61.  ,  61.  ,\n",
       "        61.  ,  61.  ,  61.  ,  61.  ,  61.  ,  61.05,  61.74,  62.  ,\n",
       "        62.12,  62.81,  63.  ,  63.19,  63.88,  64.  ,  64.26,  64.95,\n",
       "        65.  ,  65.99,  68.04,  69.42,  70.8 ,  72.63,  77.46,  79.94,\n",
       "        81.16,  81.85,  82.  ,  82.  ,  82.  ,  82.  ,  82.  ,  82.  ,\n",
       "        82.  ,  85.33,  91.06,  91.75,  92.  ,  92.39,  94.46,  97.04,\n",
       "        99.8 , 102.56, 103.  , 103.27, 103.96, 105.95, 109.04, 113.  ,\n",
       "       113.  , 113.82, 115.  , 115.  , 120.76, 127.51, 129.58, 130.  ,\n",
       "       131.44, 135.58, 154.6 , 168.48])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(dur, pctile_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7c1d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(45 >= np.percentile(dur, pctile_range)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98e0f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01845251903136418"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zyla_cv.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "005105b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.percentile(zyla_cv, pctile_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "916e6368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01845252, 0.02533617, 0.02929027, 0.03124474, 0.03480364,\n",
       "       0.03776052, 0.03873575, 0.0428634 , 0.04479267, 0.04672167,\n",
       "       0.04732347, 0.05217127, 0.05341476, 0.05708229, 0.05767232,\n",
       "       0.05978218, 0.06739226, 0.06767359, 0.07116653, 0.07584223])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a37bff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(0.041 >= percentiles).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "201ba55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04479267331673768"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5623544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04396471206465507"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zyla_cv[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ad8d2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "038499c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100-pctile_range[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbae07d",
   "metadata": {},
   "source": [
    "## See above for how to get percentile values to 5% level\n",
    "\n",
    "Now we'll apply this to the whole archive, writing the mean seeing percentile and duration percentile. We'll do this individually for every date in the flare, filament, psp, and qs data sets, and for the whole archive. Each file will have the date, the mean seeing percentile, the duration percentile, and the event factors. That'll give us everything we need for putting the numbers on the webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e0fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3041fd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 417/417 [00:00<00:00, 733.14it/s]\n"
     ]
    }
   ],
   "source": [
    "flare_quality = open(\"quality_control/flare_quality.txt\",\"w\")\n",
    "filament_quality = open(\"quality_control/filament_quality.txt\",\"w\")\n",
    "psp_quality = open(\"quality_control/psp_quality.txt\",\"w\")\n",
    "qs_quality = open(\"quality_control/qs_quality.txt\",\"w\")\n",
    "archive_quality = open(\"quality_control/archive_quality.txt\",\"w\")\n",
    "\n",
    "header = 'DATE,OBSTYPE,QUALITY,SEEING_FACTOR,DURATION_FACTOR,EVENT_FACTOR,EVENT_TYPE\\n'\n",
    "flare_quality.write(header)\n",
    "filament_quality.write(header)\n",
    "psp_quality.write(header)\n",
    "qs_quality.write(header)\n",
    "archive_quality.write(header)\n",
    "\n",
    "dur_keys = ['ZYLA_CV','CONT_CV','GBAND_CV','CAK_CV','IBIS_CV','SEE_CV']\n",
    "obstypes = ['flare','filament','qs','psp']\n",
    "otype_files = [flare_quality, filament_quality, qs_quality, psp_quality]\n",
    "\n",
    "for i in tqdm.tqdm(range(len(qp['DATE']))):\n",
    "    if qp['OBSTYPE'][i] != b'nan':\n",
    "        # Full-archive quality first\n",
    "        seeing_percentile = []\n",
    "        for key in dur_keys:\n",
    "            if not np.isnan(qp[key][i]):\n",
    "                full_cv = qp[key][~np.isnan(qp[key])]\n",
    "                percentiles = np.percentile(full_cv, pctile_range)\n",
    "                seeing_percentile.append(\n",
    "                    100 - pctile_range[\n",
    "                        np.argwhere(qp[key][i] >= percentiles).max()\n",
    "                    ]\n",
    "                )\n",
    "        if len(seeing_percentile) > 0:\n",
    "            seeing_factor = round(np.nanmean(seeing_percentile)/10,1)\n",
    "        else:\n",
    "            seeing_factor = np.nan\n",
    "        duration_percentile = []\n",
    "        if not np.isnan(qp['DURATION'][i]):\n",
    "            full_dur = qp['DURATION'][~np.isnan(qp['DURATION'])]\n",
    "            percentiles = np.percentile(full_dur, pctile_range)\n",
    "            duration_percentile.append(\n",
    "                pctile_range[\n",
    "                    np.argwhere(qp['DURATION'][i] >= percentiles).max()\n",
    "                ]\n",
    "            )\n",
    "        if len(duration_percentile) > 0:\n",
    "            duration_factor = duration_percentile[0]/10\n",
    "        else:\n",
    "            duration_factor = np.nan\n",
    "        if (not np.isnan(seeing_factor)) & (not np.isnan(duration_factor)):\n",
    "            quality = round(np.nanmean([seeing_factor,duration_factor]) + qp['EVENT_FACTOR'][i],1)\n",
    "        else:\n",
    "            quality = np.nan\n",
    "        write_list = [\n",
    "            qp['DATE'][i],\n",
    "            qp['OBSTYPE'][i],\n",
    "            str(quality),\n",
    "            str(seeing_factor),\n",
    "            str(duration_factor),\n",
    "            str(qp['EVENT_FACTOR'][i]),\n",
    "            qp['EVENT_TYPE'][i]\n",
    "        ]\n",
    "        write_str = ','.join(write_list) + \"\\n\"\n",
    "        archive_quality.write(write_str)\n",
    "        \n",
    "        # Now we do it all the obstypes...\n",
    "        for j in range(len(obstypes)):\n",
    "            if qp['OBSTYPE'][i] == obstypes[j]:\n",
    "                seeing_percentile = []\n",
    "                for key in dur_keys:\n",
    "                    if not np.isnan(qp[key][i]):\n",
    "                        full_cv = qp[key][qp['OBSTYPE'] == obstypes[j]]\n",
    "                        full_cv = full_cv[~np.isnan(full_cv)]\n",
    "                        percentiles = np.percentile(full_cv, pctile_range)\n",
    "                        seeing_percentile.append(\n",
    "                            100 - pctile_range[\n",
    "                                np.argwhere(qp[key][i] >= percentiles).max()\n",
    "                            ]\n",
    "                        )\n",
    "                if len(seeing_percentile) > 0:\n",
    "                    seeing_factor = round(np.nanmean(seeing_percentile)/10,1)\n",
    "                else:\n",
    "                    seeing_factor = np.nan\n",
    "                duration_percentile = []\n",
    "                if not np.isnan(qp['DURATION'][i]):\n",
    "                    full_dur = qp['DURATION'][qp['OBSTYPE'] == obstypes[j]]\n",
    "                    full_dur = full_dur[~np.isnan(full_dur)]\n",
    "                    percentiles = np.percentile(full_dur, pctile_range)\n",
    "                    duration_percentile.append(\n",
    "                        pctile_range[\n",
    "                            np.argwhere(qp['DURATION'][i] >= percentiles).max()\n",
    "                        ]\n",
    "                    )\n",
    "                if len(duration_percentile) > 0:\n",
    "                    duration_factor = duration_percentile[0]/10\n",
    "                else:\n",
    "                    duration_factor = np.nan\n",
    "                if (not np.isnan(seeing_factor)) & (not np.isnan(duration_factor)):\n",
    "                    quality = round(np.nanmean([seeing_factor,duration_factor]) + qp['EVENT_FACTOR'][i], 2)\n",
    "                else:\n",
    "                    quality = np.nan\n",
    "                write_list = [\n",
    "                    qp['DATE'][i],\n",
    "                    qp['OBSTYPE'][i],\n",
    "                    str(quality),\n",
    "                    str(seeing_factor),\n",
    "                    str(duration_factor),\n",
    "                    str(qp['EVENT_FACTOR'][i]),\n",
    "                    qp['EVENT_TYPE'][i]\n",
    "                ]\n",
    "                write_str = ','.join(write_list) + \"\\n\"\n",
    "                otype_files[j].write(write_str)\n",
    "                \n",
    "for file in otype_files:\n",
    "    file.close()\n",
    "archive_quality.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca7eff24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.86289350e-05, 5.43909170e-04, 7.42749427e-04, 9.64981600e-04,\n",
       "       1.26283296e-03, 1.52657664e-03, 2.08916300e-03, 2.38100446e-03,\n",
       "       2.61023097e-03, 3.08866956e-03, 3.41000345e-03, 4.20144756e-03,\n",
       "       4.67762175e-03, 5.12932094e-03, 5.54840034e-03, 6.48989595e-03,\n",
       "       6.89874773e-03, 7.27481020e-03, 8.21661970e-03, 1.02483302e-02])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c34b921f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015681340392054124"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp[key][i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
